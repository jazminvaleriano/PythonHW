{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Week 4 - Assignment</center></h2>\n",
    "<h3><center>Programming for Data Science 2024</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises for the topics covered in the fourth lecture.\n",
    "\n",
    "The exercise will be marked as passed if you get **at least 10/15** points.\n",
    "\n",
    "Exercises must be handed in via **ILIAS** (Homework assignments). Deliver your submission as a compressed file (zip) containing one .py or .ipynb file with all exercises. The name of both the .zip and the .py/.ipynb file **must** be *SurnameName* of the two members of the group. Example: Riccardo Cusinato + Athina Tzovara = *CusinatoRiccardo_TzovaraAthina.zip* .\n",
    "\n",
    "It's important to use comments to explain your code and show that you're able to take ownership of the exercises and discuss them.\n",
    "\n",
    "You are not expected to collaborate outside of the group on exercises and submitting other groupsâ€™ code as your own will result in 0 points.\n",
    "\n",
    "For questions contact: *riccardo.cusinato@unibe.ch* with the subject: *Programming for Data Science 2024*.\n",
    "\n",
    "**Deadline: 14:00, March 21, 2024.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:left;\">Exercise 1 - Create Dataframes<span style=\"float: right\">5 points</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame *episodes_df* with the columns **ses**, **ep**, and **title**, as below:\n",
    "\n",
    "|**ses**|**ep**|**title**|\n",
    "|:---|:---|:---|\n",
    "|1|1|One|\n",
    "|1|2|Two|\n",
    "|2|1|Three|\n",
    "|2|2|Four|\n",
    "\n",
    "Create a DataFrame *imdb_df* with the columns **ses**, **ep**, and **score**, as below:\n",
    "\n",
    "|**ses**|**ep**|**score**|\n",
    "|:---|:---|:---|\n",
    "|1|1|8.4|\n",
    "|1|2|8.1|\n",
    "|2|1|7.9|\n",
    "|2|2|7.7|\n",
    "\n",
    "Merge the two DataFrames. Then, find and print the title of the episode with the higest score.\n",
    "\n",
    "**NB**: To merge the two dataframes you have to use the *merge* method:\n",
    "```python\n",
    "merged_df = episodes_df.merge(imdb_df, on=['ses', 'ep'])\n",
    "```\n",
    "\n",
    "\n",
    "1. By manipulating the dataframes, find and print the title of the episode with the higest score. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the episode with the highest score: One\n",
      "   ses  ep  title  score\n",
      "0    1   1    One    8.4\n",
      "1    1   2    Two    8.1\n",
      "2    2   1  Three    7.9\n",
      "3    2   2   Four    7.7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating DataFrame episodes_df\n",
    "episodes_df = pd.DataFrame({\n",
    "    'ses': [1, 1, 2, 2],\n",
    "    'ep': [1, 2, 1, 2],\n",
    "    'title': ['One', 'Two', 'Three', 'Four']\n",
    "})\n",
    "\n",
    "# Creating DataFrame imdb_df\n",
    "imdb_df = pd.DataFrame({\n",
    "    'ses': [1, 1, 2, 2],\n",
    "    'ep': [1, 2, 1, 2],\n",
    "    'score': [8.4, 8.1, 7.9, 7.7]\n",
    "})\n",
    "\n",
    "# Merging the two DataFrames\n",
    "merged_df = episodes_df.merge(imdb_df, on=['ses', 'ep'])\n",
    "\n",
    "# Using the idxmax() function to find the index of the row with the maximum value in the 'score' column\n",
    "max_score_index = merged_df['score'].idxmax()\n",
    "\n",
    "# Using the loc[] function to locate the row with the max_score_index \n",
    "highest_score_row = merged_df.loc[max_score_index]\n",
    "\n",
    "highest_score_title = highest_score_row['title']\n",
    "\n",
    "print(\"Title of the episode with the highest score:\", highest_score_title)\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Change the **score** of the entry with the title \"Three\" in the DataFrame you created in Task 1 and print the result. The new score should be 6. (*2 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ses</th>\n",
       "      <th>ep</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>One</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Two</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Three</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Four</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ses  ep  title  score\n",
       "0    1   1    One    8.4\n",
       "1    1   2    Two    8.1\n",
       "2    2   1  Three    6.0\n",
       "3    2   2   Four    7.7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Using .loc method to modify the 'score' value of the element with the title 'Three' using array-like index:\n",
    "merged_df.loc[merged_df.title=='Three','score']=6\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:left;\">Exercise 2 - Load DataFrames<span style=\"float: right\">6 points</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the two CSV files 'silicon_valley_episodes.csv' and 'silicon_valley_imdb.csv', found in the \"data\" directory, as DataFrames. Merge the two DataFrames as in the first task, using **season** and **episode_num** as keys to merge on. (*2 points*)\n",
    "```python\n",
    "episodes_df = pd.read_csv('./data/silicon_valley_episodes.csv')\n",
    "imdb_df = pd.read_csv('./data/silicon_valley_imdb.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/silicon_valley_episodes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Loading the CSV files into DataFrames\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m episodes_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/silicon_valley_episodes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m imdb_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/silicon_valley_imdb.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Merging the two DataFrames using season and episode_num as keys\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/silicon_valley_episodes.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the CSV files into DataFrames\n",
    "episodes_df = pd.read_csv('./data/silicon_valley_episodes.csv')\n",
    "imdb_df = pd.read_csv('./data/silicon_valley_imdb.csv')\n",
    "\n",
    "# Merging the two DataFrames using season and episode_num as keys\n",
    "merged_df = episodes_df.merge(imdb_df, on=['season', 'episode_num'])\n",
    "\n",
    "# Displaying the merged DataFrame\n",
    "print(merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new DataFrame, called **df_best**, containing only the episodes with an **imdb_rating** at or above 9. Use the DataFrame created in the previous task as a starting point. (*2 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    season  episode_num  episode_num_overall                        title_x  \\\n",
      "7        1            8                    8  Optimal Tip-To-Tip Efficiency   \n",
      "17       2           10                   18         Two Days of the Condor   \n",
      "45       5            8                   46              Fifty-One Percent   \n",
      "\n",
      "   directed_by written_by original_air_date_x  us_viewers  \\\n",
      "7   Mike Judge  Alec Berg            01.06.14     1740000   \n",
      "17   Alec Berg  Alec Berg            14.06.15     2110000   \n",
      "45   Alec Berg  Alec Berg            13.05.18      707000   \n",
      "\n",
      "                          title_y original_air_date_y  imdb_rating  \\\n",
      "7   Optimal Tip-To-Tip Efficiency         1 Jun. 2014          9.3   \n",
      "17         Two Days of the Condor        14 Jun. 2015          9.2   \n",
      "45              Fifty-One Percent         13 May 2018          9.1   \n",
      "\n",
      "    total_votes                                               desc  \n",
      "7          3627  Pied Piper makes it to the next stage of Disru...  \n",
      "17         2320  The verdict on Pied Piper's fate coincides wit...  \n",
      "45         2128  PiperNet launches, but an early success makes ...  \n"
     ]
    }
   ],
   "source": [
    "# Filtering episodes with IMDb rating at or above 9 from the merged_df\n",
    "df_best = merged_df[merged_df['imdb_rating'] >= 9]\n",
    "\n",
    "# Displaying the DataFrame containing only the best episodes\n",
    "print(df_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find mean number of **us_viewers** for episodes with an IMDB score greater than or equal to 9, and for episodes with an IMDB score lower than 9, and print the means. (*2 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of US viewers for episodes with IMDb score greater than or equal to 9: 1519000.0\n",
      "Mean number of US viewers for episodes with IMDb score lower than 9: 1234000.0\n"
     ]
    }
   ],
   "source": [
    "# Filtering DataFrame for episodes with IMDb score greater than or equal to 9\n",
    "df_rating_high = merged_df[merged_df['imdb_rating'] >= 9]\n",
    "\n",
    "# Calculating the mean number of US viewers for episodes with IMDb score greater than or equal to 9\n",
    "mean_us_viewers_high_rating = df_rating_high['us_viewers'].mean()\n",
    "\n",
    "# Filtering DataFrame for episodes with IMDb score lower than 9\n",
    "df_rating_low = merged_df[merged_df['imdb_rating'] < 9]\n",
    "\n",
    "# Calculating the mean number of US viewers for episodes with IMDb score lower than 9\n",
    "mean_us_viewers_low_rating = df_rating_low['us_viewers'].mean()\n",
    "\n",
    "# Printing the means\n",
    "print(\"Mean number of US viewers for episodes with IMDb score greater than or equal to 9:\", mean_us_viewers_high_rating)\n",
    "print(\"Mean number of US viewers for episodes with IMDb score lower than 9:\", mean_us_viewers_low_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:left;\">Exercise 3 - DataFrames Ufuncs<span style=\"float: right\">4 points</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the two dataframe *df1* and *df2* with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame(\n",
    "    np.arange(1, 10).reshape(3, 3),\n",
    "    columns=[\"a\", \"b\", \"c\"],\n",
    "    index=[\"1\", \"2\", \"3\"]\n",
    ")\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    np.arange(1, 10).reshape(3, 3) / 2,\n",
    "    columns=[\"a\", \"b\", \"d\"],\n",
    "    index=[\"1\", \"2\", \"4\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add the two dataframes together, with the appropriate pandas method, and print the result. (*0.5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    b   c   d\n",
      "1  1.5  3.0 NaN NaN\n",
      "2  6.0  7.5 NaN NaN\n",
      "3  NaN  NaN NaN NaN\n",
      "4  NaN  NaN NaN NaN\n"
     ]
    }
   ],
   "source": [
    "# Printing the result of adding the 2 dataframes:\n",
    "print(df1 + df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add the underlying numpy objects of the two dataframes, and print the result. (*0.5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5  3.   4.5]\n",
      " [ 6.   7.5  9. ]\n",
      " [10.5 12.  13.5]]\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the underlying numpy objects of the 2 dataframes:\n",
    "array1 = df1.to_numpy()\n",
    "array2 = df2.to_numpy()\n",
    "\n",
    "# Adding them together and printing the result: \n",
    "print(array1 + array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare the two results that you obtained and comment if and **why** they are different. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# When working with dataframes (pandas), the operation is executed only on matching index/column,\n",
    "# and the result contains all the keys (a, b, c and d). Where the column key don't match, pandas will \n",
    "# fill in with NaN. \n",
    "# When operating on the numpy arrays, as they have the same shape (3,3), the operation will  \n",
    "# be exexuted element-wise, and the addition result will keep the shape 3,3. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
